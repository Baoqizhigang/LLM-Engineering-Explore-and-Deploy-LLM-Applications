{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62ec465-1e7b-4a92-bf18-19113b7ab2ff",
   "metadata": {},
   "source": [
    "# Let's go multi-modal!!\n",
    "\n",
    "We can use DALL-E-3, the image generation model behind GPT-4o, to make us some images\n",
    "\n",
    "Let's put this in a function called artist.\n",
    "\n",
    "### Price alert: each time I generate an image it costs about 4c - don't go crazy with images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e9218d-25fb-4904-9d22-9d8ba055ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33312c-7875-4b5d-b5df-fc3d404c130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d556d-d3e2-4978-9438-455333a3a1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant for an Airline called FlightAI. \"\n",
    "system_message += \"Give short, courteous answers, no more than 1 sentence. \"\n",
    "system_message += \"Always be accurate. If you don't know the answer, say so.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334ea47-4068-494b-b47e-1219473424b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by making a useful function\n",
    "\n",
    "ticket_prices = {\"london\": \"$799\", \"paris\": \"$899\", \"tokyo\": \"$1400\", \"berlin\": \"$499\"}\n",
    "\n",
    "def get_ticket_price(destination_city):\n",
    "    print(f\"Tool get_ticket_price called for {destination_city}\")\n",
    "    city = destination_city.lower()\n",
    "    return ticket_prices.get(city, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb3589-cde4-43d1-93bf-db136cffd009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a particular dictionary structure that's required to describe our function:\n",
    "\n",
    "price_function = {\n",
    "    # Identify the function by name, allowing the AI model to recognize and invoke it directly.\n",
    "    \"name\" : \"get_ticket_price\",\n",
    "    # \"description\": Provides an explanation of when and why the function should be used, guiding the model to call it specifically when ticket price information is requested.\n",
    "    \"description\" : \"Get the price of a return ticket to the destination city. Call this whenever you need to know the ticket price, for example when a customer asks 'How much is a ticket to this city'\",\n",
    "    # \"parameters\": Defines the structure of inputs required by the function\n",
    "    \"parameters\" : {\n",
    "        \"type\" : \"object\", # Specify that the parameters will be in object form\n",
    "        #\"properties\":  Lists required fields within the function’s parameters.\n",
    "        \"properties\" : {\n",
    "            # A required field describing the destination for which the user seeks a ticket price.\n",
    "            \"destination_city\" : {\n",
    "            \"type\" : \"string\",\n",
    "            \"description\" : \"The city that the customer wants to travel to\",\n",
    "        },\n",
    "    },\n",
    "    # Ensure that destination_city is provided whenever this function is called, as it’s essential for retrieving the price.\n",
    "    \"required\" : [\"destination_city\"],\n",
    "    # Restrict inputs to just the specified parameters, improving reliability.\n",
    "    \"additionalProperties\" : False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fdd8e-0f13-404b-b9ae-5bc26ba6f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This list of tools provides a standardized way for an AI model to access various functions it might use during interactions.\n",
    "# \"type\": \"function\": Specifies that each item in the list is a function type, indicating the role of the item.\n",
    "# \"function\": price_function: Associates the actual function dictionary (price_function) with the tool.\n",
    "tools = [{\"type\" : \"function\", \"function\" : price_function}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c17ec17-5e52-44f7-8957-e056214e7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "\n",
    "    if response.choices[0].finish_reason==\"tool_calls\":\n",
    "        message = response.choices[0].message\n",
    "        response, city = handle_tool_call(message)\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7487afe-f262-490e-8c39-03e8bd8e64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to write that function handle_tool_call:\n",
    "\n",
    "def handle_tool_call(message):\n",
    "    tool_call = message.tool_calls[0]\n",
    "    arguments = json.loads(tool_call.function.arguments)\n",
    "    city = arguments.get('destination_city')\n",
    "    price = get_ticket_price(city)\n",
    "    response = {\n",
    "        \"role\": \"tool\",\n",
    "        \"content\": json.dumps({\"destination_city\": city,\"price\": price}),\n",
    "        \"tool_call_id\": message.tool_calls[0].id\n",
    "    }\n",
    "    return response, city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b6ff2-a4d1-4ac8-b19d-dfc2e38065e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports for handling images\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733c0e7-0e2c-4345-9693-86d1258e14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def artist(city):\n",
    "    image_response = openai.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city}, in a vibrant pop-art style\",\n",
    "            size=\"1024x1024\",\n",
    "            n=1,\n",
    "            response_format=\"b64_json\",\n",
    "        )\n",
    "    image_base64 = image_response.data[0].b64_json\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    return Image.open(BytesIO(image_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbe3dc-76b5-472f-a519-c1aff78e7598",
   "metadata": {},
   "source": [
    "## Audio\n",
    "\n",
    "And let's make a function talker that uses OpenAI's speech model to generate Audio\n",
    "\n",
    "### Troubleshooting Audio issues\n",
    "\n",
    "If you have any problems running this code below (like a FileNotFound error, or a warning of a missing package), you may need to install FFmpeg, a very popular audio utility.\n",
    "\n",
    "**For PC Users**\n",
    "\n",
    "1. Download FFmpeg from the official website: https://ffmpeg.org/download.html\n",
    "\n",
    "2. Extract the downloaded files to a location on your computer (e.g., `C:\\ffmpeg`)\n",
    "\n",
    "3. Add the FFmpeg bin folder to your system PATH:\n",
    "- Right-click on 'This PC' or 'My Computer' and select 'Properties'\n",
    "- Click on 'Advanced system settings'\n",
    "- Click on 'Environment Variables'\n",
    "- Under 'System variables', find and edit 'Path'\n",
    "- Add a new entry with the path to your FFmpeg bin folder (e.g., `C:\\ffmpeg\\bin`)\n",
    "- Restart your command prompt, and within Jupyter Lab do Kernel -> Restart kernel, to pick up the changes\n",
    "\n",
    "4. Open a new command prompt and run this to make sure it's installed OK\n",
    "`ffmpeg -version`\n",
    "\n",
    "**For Mac Users**\n",
    "\n",
    "1. Install homebrew if you don't have it already by running this in a Terminal window and following any instructions:  \n",
    "`/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"`\n",
    "\n",
    "2. Then install FFmpeg with `brew install ffmpeg`\n",
    "\n",
    "3. Verify your installation with `ffmpeg -version` and if everything is good, within Jupyter Lab do Kernel -> Restart kernel to pick up the changes\n",
    "\n",
    "Message me or email me at ed@edwarddonner.com with any problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa285877-10c4-41f4-9d47-3fccef076aa6",
   "metadata": {},
   "source": [
    "# For Mac users\n",
    "\n",
    "This version should work fine for you. It might work for Windows users too, but you might get a Permissions error writing to a temp file. If so, see the next section!\n",
    "\n",
    "As always, if you have problems, please contact me! (You could also comment out the audio talker() in the later code if you're less interested in audio generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a3a84-fd6e-49ac-8eef-f6b60b53864f",
   "metadata": {},
   "source": [
    "When you call **talker(\"Well, hi there\")**, here’s what happens step-by-ste\n",
    "### **1. Calling the Function:**\n",
    "* \"Well, hi there\" is passed to the **talker** function as the **message** parameter.\n",
    "### **2. Generating Speech:**\n",
    "* The function sends \"Well, hi there\" to OpenAI’s Text-to-Speech (TTS) API using the specified **tts-1** model with the **\"onyx\"** voice.\n",
    "* OpenAI’s API processes this text to create an audio file, and the **response** object contains the audio data in binary form\n",
    "### **3.Converting Audio Data for Playback:**\n",
    "* **BytesIO(response.content)** wraps the binary audio data in an in-memory file-like object.\n",
    "* **AudioSegment.from_file(audio_stream, format=\"mp3\")** then converts this object into an **AudioSegment**, allowing easy handling and playback.  \n",
    "### **4. Playing the Audio:**\n",
    "* The **play** function outputs the audio through the default audio device on your system, so you should hear the phrase **\"Well, hi there\"** spoken in the selected **\"onyx\"** voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46669281-2155-438e-885a-b5ba4c88d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioSegment: This class from the pydub library provides tools for loading, manipulating, and saving audio files. \n",
    "# Here, it's used to handle audio data returned from OpenAI’s API.\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# play: This function from pydub.playback plays audio directly through the system’s default audio output.\n",
    "# It's used here to immediately play the generated audio without needing to save it to disk.\n",
    "from pydub.playback import play\n",
    "\n",
    "def talker(message):\n",
    "    # openai.audio.speech.create(): This function call sends a request to OpenAI’s API to convert text into speech audio. \n",
    "    response = openai.audio.speech.create(\n",
    "        model = \"tts-1\", # tts-1 is a model optimized for producing natural-sounding speech.\n",
    "        # voice=\"onyx\": Selects a specific voice style for the generated speech.\n",
    "        voice = \"onyx\",  # alloy onyx\n",
    "        input = message # Send the message text as input to be converted into speech.\n",
    "    )\n",
    "    # response.content: This contains the audio data returned by the TTS API in binary format, ready for playback or further manipulation.\n",
    "    # This enables loading the data without needing to save it as an intermediate file.\n",
    "    audio_stream = BytesIO(response.content)\n",
    "\n",
    "    # Load the audio data from the BytesIO stream into an AudioSegment object, specifying the format as \"mp3\".\n",
    "    # This format specification aligns with the format returned by the OpenAI TTS API.\n",
    "    audio = AudioSegment.from_file(audio_stream, format = \"mp3\")\n",
    "    \n",
    "    play(audio) # Play the AudioSegment object directly through the system’s speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97572bc4-13dd-41e2-95e3-2476b56f45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "talker(\"Well, hi there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063b0af8-64ec-4ec0-94f1-670862e72a27",
   "metadata": {},
   "source": [
    "# For Windows users\n",
    "\n",
    "## if you get a permissions error writing to a temp file, then this code should work instead.\n",
    "\n",
    "A collaboration between student Mark M. and Claude got this resolved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653ea61-cec4-4bb1-9ff2-112eb82a6b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AudioSegment: This class from the pydub library provides tools for loading, manipulating, and saving audio files. \n",
    "# Here, it's used to handle audio data returned from OpenAI’s API.\n",
    "# from pydub import AudioSegment\n",
    "\n",
    "# play: This function from pydub.playback plays audio directly through the system’s default audio output.\n",
    "# It's used here to immediately play the generated audio without needing to save it to disk.\n",
    "# from pydub.playback import play\n",
    "\n",
    "#  This module provides tools for creating temporary files and directories. \n",
    "# Here, it's used to create a temporary location for saving audio data as a .wav file.\n",
    "import tempfile\n",
    "\n",
    "# This module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. \n",
    "# Here, it’s used to call an external audio player (ffplay) to play the audio.\n",
    "import subprocess\n",
    "\n",
    "def play_audio(audio_segment):\n",
    "    # Retrieve the system’s temporary directory, ensuring a safe location for the audio file.\n",
    "    temp_dir = tempfile.gettempdir() \n",
    "    # Create the full path for the temporary audio file within the temp directory.\n",
    "    temp_path = os.path.join(temp_dir, \"temp_audio.wav\")\n",
    "    try:\n",
    "        # Export the AudioSegment as a .wav file to temp_path. This format works well with ffplay.\n",
    "        audio_segment.export(temp_path, format=\"wav\")\n",
    "        # Invokes ffplay, an audio player included with ffmpeg, to play the .wav file without displaying a window (-nodisp)\n",
    "        # and automatically closing after playback (-autoexit). Redirecting stdout and stderr to DEVNULL silences any output.\n",
    "        subprocess.call([\n",
    "            \"ffplay\",\n",
    "            \"-nodisp\",\n",
    "            \"-autoexit\",\n",
    "            \"-hide_banner\",\n",
    "            temp_path\n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    # finally block: After playback, the finally block deletes the temporary file (temp_path). \n",
    "    # If an error occurs during deletion, it’s safely ignored\n",
    "    finally:\n",
    "        try:\n",
    "            os.remove(temp_path)\n",
    "        except Exception:\n",
    "            pass\n",
    " \n",
    "\n",
    "def talker(message):\n",
    "    # openai.audio.speech.create(): This function call sends a request to OpenAI’s API to convert text into speech audio. \n",
    "    response = openai.audio.speech.create(\n",
    "        model = \"tts-1\", # tts-1 is a model optimized for producing natural-sounding speech.\n",
    "        # voice=\"onyx\": Selects a specific voice style for the generated speech.\n",
    "        voice = \"onyx\",  # alloy onyx\n",
    "        input = message # Send the message text as input to be converted into speech.\n",
    "    )\n",
    "    # response.content: This contains the audio data returned by the TTS API in binary format, ready for playback or further manipulation.\n",
    "    # This enables loading the data without needing to save it as an intermediate file.\n",
    "    audio_stream = BytesIO(response.content)\n",
    "\n",
    "    # Load the audio data from the BytesIO stream into an AudioSegment object, specifying the format as \"mp3\".\n",
    "    # This format specification aligns with the format returned by the OpenAI TTS API.\n",
    "    audio = AudioSegment.from_file(audio_stream, format = \"mp3\")\n",
    "    \n",
    "    play_audio(audio) # Play the AudioSegment object directly through the system’s speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ede49-e4ac-470c-aa6f-439161043732",
   "metadata": {},
   "outputs": [],
   "source": [
    "talker(\"Well hi there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b2f9ff-912a-4a1d-a91a-8e162cc50930",
   "metadata": {},
   "source": [
    "# Our Agent Framework\n",
    "\n",
    "The term 'Agentic AI' and Agentization is an umbrella term that refers to a number of techniques, such as:\n",
    "\n",
    "1. Breaking a complex problem into smaller steps, with multiple LLMs carrying out specialized tasks\n",
    "2. The ability for LLMs to use Tools to give them additional capabilities\n",
    "3. The 'Agent Environment' which allows Agents to collaborate\n",
    "4. An LLM can act as the Planner, dividing bigger tasks into smaller ones for the specialists\n",
    "5. The concept of an Agent having autonomy / agency, beyond just responding to a prompt - such as Memory\n",
    "\n",
    "We're showing 1 and 2 here, and to a lesser extent 3 and 5. In week 8 we will do the lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf79c6-5b82-42f5-93f4-53edc55fc70b",
   "metadata": {},
   "source": [
    "### **Purpose of Each Step**\n",
    "* **Context Management:** Adds both system and previous user-assistant messages to maintain conversation flow.\n",
    "* **Response Generation:** Uses OpenAI’s chat API to produce responses, with optional tool support.\n",
    "* **Dynamic Tool Calls:** Triggers additional actions (like generating an image) when appropriate, providing richer responses.\n",
    "* **Speech Integration:** Converts text to speech, making the interaction more engaging and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9e408-bf7a-4c49-b181-e2af350d8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(history):\n",
    "    # Combines a system message (defined by system_message) with the history of previous messages. \n",
    "    # The system message provides instructions or context for the AI, such as conversational tone or focus areas.\n",
    "    # history here includes all prior interactions (both user and assistant messages), creating a conversational context for generating coherent responses.\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "\n",
    "    # Call OpenAI’s API to generate a response\n",
    "    # tools=tools: Indicates any tools (like image generation or TTS) that the assistant can invoke.\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages, tools=tools)\n",
    "    \n",
    "    # Initialize image as None. It will be set to an image if the response triggers a tool call for image generation.\n",
    "    image = None\n",
    "\n",
    "    # Checks if the assistant’s response requires a tool, such as DALL-E for generating an image.\n",
    "    if response.choices[0].finish_reason==\"tool_calls\": \n",
    "        message = response.choices[0].message # Capture the tool call message.\n",
    "        # The handle_tool_call function processes the tool call and retrieves additional information (e.g., city).\n",
    "        response, city = handle_tool_call(message)\n",
    "        # Adds the tool call message and its response to messages to maintain conversation continuity.\n",
    "        messages.append(message)\n",
    "        messages.append(response)\n",
    "        # Calls the artist function with city, generating an image for this context. image is updated with this result.\n",
    "        image = artist(city)\n",
    "        # Calls the API again, this time including the tool call and image in the conversation history, to generate a follow-up response.\n",
    "        response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "   \n",
    "    # Retrieve the assistant’s final reply.    \n",
    "    reply = response.choices[0].message.content \n",
    "    # Update history with the assistant's response.\n",
    "    history += [{\"role\":\"assistant\", \"content\":reply}]\n",
    "    # Call talker to convert reply to speech, enhancing the interactive experience with spoken feedback.\n",
    "    talker(reply)\n",
    "    # Return the updated conversation history and any generated image. If no image was created, image remains None.\n",
    "    return history, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302124fe-fe6f-40f8-b8d7-c3992c4eb2d1",
   "metadata": {},
   "source": [
    "###  **Purpose of Each Component**\n",
    "\n",
    "This setup provides a tailored, interactive AI assistant experience with multi-modal capabilities. \n",
    "\n",
    "* **Custom Chat Interface:** Creates a flexible, multi-modal chat UI that includes both text and image outputs, bypassing Gradio’s preset interface.\n",
    "* **Function Chaining:** Uses Gradio’s **.submit()** and **.then()** methods to process user input, update history, call the assistant for a response, and display output in a seamless, interactive flow.\n",
    "* **Clear Button:** Allows users to reset the conversation quickly, improving usability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d01be45-2b33-4790-96e3-362cfc1d96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More involved Gradio code as we're not using the preset Chat interface!\n",
    "# Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.\n",
    "\n",
    "# This groups UI elements, creating a cohesive layout for the custom chat interface.\n",
    "with gr.Blocks() as ui:\n",
    "    # Arrange elements horizontally within each row.\n",
    "    with gr.Row():\n",
    "        # Create a chat display with a fixed height of 500 pixels and sets type=\"messages\", ensuring messages appear in a conversational format.\n",
    "        chatbot = gr.Chatbot(height=500, type=\"messages\")\n",
    "        # Provide an image display beside the chat, which can be used to show images generated by the assistant.\n",
    "        image_output = gr.Image(height=500)\n",
    "    with gr.Row():\n",
    "        # Add a textbox where users can type messages, labeled for clarity.\n",
    "        entry = gr.Textbox(label=\"Chat with our AI Assistant:\")\n",
    "    with gr.Row():\n",
    "        # Add a button labeled \"Clear\" to reset the chat history\n",
    "        clear = gr.Button(\"Clear\")\n",
    "        \n",
    "    # The function appends the user’s message to history, then returns an empty string to clear the textbox, and updates the chatbot display.\n",
    "    # message: Represents the current input from the user.\n",
    "    # history: Holds the conversation history.\n",
    "    def do_entry(message, history):\n",
    "        history += [{\"role\":\"user\", \"content\":message}]\n",
    "        return \"\", history\n",
    "\n",
    "    # Configures what happens when the user submits a message\n",
    "    # do_entry: First, the do_entry function updates history with the new user message.\n",
    "    # After do_entry completes, the chat function is called, using chatbot as input. \n",
    "    # chat processes the conversation history and generates an assistant response, \n",
    "    # which appears in the chatbot and updates the image_output if an image is generated.\n",
    "    entry.submit(do_entry, inputs=[entry, chatbot], outputs=[entry, chatbot]).then(\n",
    "        chat, inputs=chatbot, outputs=[chatbot, image_output]\n",
    "    )\n",
    "    # This line links the \"Clear\" button to a function that resets the chatbot display, effectively clearing the chat history. \n",
    "    # The queue=False setting ensures the reset action occurs immediately.\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "# Launche the Gradio UI and opens it in a new browser tab, enabling immediate access to the chat interface.\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0df953-4676-4b15-b307-0cc92c2e0e20",
   "metadata": {},
   "source": [
    "# Business Applications\n",
    "\n",
    "Add in more tools - perhaps to simulate actually booking a flight. A student has done this and provided their example in the community contributions folder.\n",
    "\n",
    "Next: take this and apply it to your business. Make a multi-modal AI assistant with tools that could carry out an activity for your work. A customer support assistant? New employee onboarding assistant? So many possibilities!\n",
    "\n",
    "If you feel bold, see if you can add audio input to our assistant so you can talk to it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

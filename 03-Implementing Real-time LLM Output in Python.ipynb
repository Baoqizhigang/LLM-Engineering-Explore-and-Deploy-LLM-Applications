{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7172e573-ea65-4291-a143-6e66c199987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b868d-917a-42ac-bd43-724f02456bbb",
   "metadata": {},
   "source": [
    "## **Anthropic_API_key**\n",
    "\n",
    "To generate an Anthropic API key, you'll need to follow these steps:\n",
    "* Go to the Anthropic website (https://www.anthropic.com)\n",
    "* Sign up for an account if you haven't already done so.\n",
    "* Once logged in, navigate to the API section or dashboard.\n",
    "* Look for an option to create or generate a new API key.\n",
    "* Follow the prompts to generate your API key.\n",
    "* Make sure to copy and securely store your API key, as you typically won't be able to view it again after initial generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6620fc76-e283-4fad-b990-ee2e9dc32b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google.generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01005904-b550-4ebd-ad48-ccbdf97f7603",
   "metadata": {},
   "source": [
    "## **GOOGLE_API_KEY**\n",
    "To access Gemini 1.5 Flash, you need to use the Google AI Studio or the Vertex AI API. Here's the correct process:\n",
    "\n",
    "* Go to Google AI Studio:\n",
    "* Visit https://makersuite.google.com/\n",
    "* Sign in with your Google account\n",
    "* Navigate to the API key section:\n",
    "* Look for an option like \"Get API key\" or \"API keys\" in the interface\n",
    "* Generate a new API key **1**: Follow the prompts to create a new API key specifically for Gemini models\n",
    "* Generate a new API key **2** :Copy and securely store your new API key\n",
    "\n",
    "This API key should allow you to access Gemini 1.5 Flash through the appropriate Google AI endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f4d8ab-d060-4267-854e-d09624a6e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code imports essential libraries and tools needed to build a real-time LLM output system in Python, particularly for use in a Jupyter Notebook environment. \n",
    "#It sets up access to multiple LLM APIs (OpenAI, Google Generative AI, and Anthropic) and ensures that sensitive credentials are securely managed. \n",
    "#Additionally, it provides tools to display and update outputs dynamically, making it ideal for a responsive and interactive user experience.\n",
    "\n",
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# This import provides access to Google’s Generative AI tools, which allow interaction with Google’s LLM services. \n",
    "# Similar to OpenAI’s API, Google Generative AI offers models for generating text and performing other NLP tasks.\n",
    "import google.generativeai\n",
    "\n",
    "# This imports the anthropic library, which provides access to models developed by Anthropic, such as Claude. \n",
    "# Anthropic focuses on building models with safety and alignment as their key principles.\n",
    "import anthropic\n",
    "\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa348c17-85f6-492d-9283-159cafefc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables defined in the .env file into the current Python environment.\n",
    "load_dotenv()\n",
    "\n",
    "# This line ensures that the OpenAI API key is stored in the os.environ dictionary. \n",
    "# This dictionary holds all environment variables, which can be accessed by the script at runtime.\n",
    "# If the key is not found in the environment, it defaults to 'your-key-if-not-using-env'.\n",
    "# os.environ['OPENAI_API_KEY']: This explicitly sets the environment variable OPENAI_API_KEY to be used by the OpenAI client.\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY','your-key-if-not-using-env')\n",
    "\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe407df0-fa23-4207-a117-c7dbc7be1101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "# All 3 APIs are similar\n",
    "# Having problems with API files? You can use openai = OpenAI(api_key=\"your-key-here\") and same for claude\n",
    "# Having problems with Google Gemini setup? Then just skip Gemini; you'll get all the experience you need from GPT and Claude.\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dcae50-c1bd-428c-880b-8d859da253bf",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "083240f2-0924-4921-baac-39aa64fb6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system message defines the behavior, personality, or context for the assistant (the LLM). \n",
    "# It is used to set up the role or persona that the LLM should assume throughout the interaction.\n",
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "\n",
    "# The user prompt specifies what the user wants from the assistant (the LLM). \n",
    "# It’s the input that the model will directly respond to based on the context established by the system message.\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c98cf-31f2-462f-8506-3fb7c14e6b89",
   "metadata": {},
   "source": [
    "### **Creating the prompts List**\n",
    "* The model processes the prompts list in sequence.\n",
    "* First, it understands its role based on the \"system\" message.\n",
    "* Then it processes the \"user\" prompt to generate an appropriate response.\n",
    "* This structure ensures that the model knows both how to respond and what to respond to, creating a more consistent and contextually accurate output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e08c1c-d479-4148-90d2-204fff9dce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prompts list is an organized structure that combines the system message and user prompt into a single input for the LLM API. \n",
    "# This structure is used to communicate with models like GPT-4 and is necessary for creating a coherent conversational context.\n",
    "# \"role\": Specifies the type of message, such as \"system\", \"user\", or \"assistant\".\n",
    "# \"system\": Sets the initial context and behavior of the LLM.\n",
    "# \"user\": Provides the user’s prompt or question.\n",
    "# \"content\": Contains the actual text of the message (system instructions or user query).\n",
    "prompts = [\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":\"user\",\"content\":user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c79ff5-f2cb-40f6-99f0-5adb88a32634",
   "metadata": {},
   "source": [
    "## **Technical Details and Considerations**\n",
    "\n",
    "### **API Call Structure:**\n",
    "\n",
    "* The OpenAI API uses a conversational model structure where you provide a sequence of messages with different roles (system, user, and assistant). This helps the model maintain context and respond appropriately based on its \"role.\"\n",
    "\n",
    "### **Model Choice:**\n",
    "\n",
    "* **gpt-3.5-turbo** is chosen for efficiency and cost-effectiveness compared to other models like GPT-4. It’s designed to handle conversational tasks, making it suitable for generating interactive and engaging responses like jokes.\n",
    "\n",
    "### **Accessing and Displaying Content:** \n",
    "\n",
    "* The API returns a response in a structured format (JSON-like), allowing easy access to different parts of the output, such as the message text and other metadata.\n",
    "* Only the relevant part (the joke) is extracted and displayed using **print()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06299829-997b-4653-a627-02fc37491fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the statistician so bad at relationships?\n",
      "\n",
      "Because he couldn't find a significant other!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "\n",
    "# Call the OpenAI API to generate a response from the GPT-3.5-Turbo model.\n",
    "# model='gpt-3.5-turbo': Specifies the model to use. \n",
    "# messages=prompts: Passes the structured conversation (prompts) containing the system message and user prompt to the API.\n",
    "# The model interprets the system message to understand its role (telling jokes) and ...\n",
    "# then reads the user prompt to generate an appropriate response based on its knowledge and instructions.\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "\n",
    "# Accessing the Response\n",
    "# completion.choices[0].message.content:\n",
    "# completion: The variable storing the output returned by the OpenAI API. It contains a lot of information, including the response text and metadata.\n",
    "# choices: A list of response choices generated by the model. In most cases, there is only one choice (i.e., choices[0]), but multiple responses could be returned depending on the API settings.\n",
    "# message.content: The actual text of the response generated by the model. This is the part that contains the joke based on the prompt and the role defined in the system message.\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4afaf-d83e-4469-ae19-dc0e51c9fd77",
   "metadata": {},
   "source": [
    "## **Parameters: temperature=0.7**\n",
    "\n",
    "This parameter controls the creativity and randomness of the model’s output. The temperature value ranges from 0 to 1 (or sometimes higher), where:\n",
    "\n",
    "* **Lower temperatures (close to 0):** Produce more deterministic and focused outputs, making the model generate responses that are likely more repetitive but reliable.\n",
    "* **Low temperature (e.g., 0.1):** The model would likely produce very similar jokes each time, as it picks the most probable continuation based on the training data.\n",
    "* **Higher temperatures (close to 1):** Produce more creative and diverse outputs, but they may be less consistent and more variable.\n",
    "* **High temperature (e.g., 0.9):** The model would vary its responses significantly, potentially generating more unusual and creative jokes, but they might also be less accurate or relevant.\n",
    "* A value of **0.7** strikes a balance between creativity and coherence, allowing the model to generate interesting but still relevant and controlled responses.\n",
    "* A balanced temperature setting often recommended for tasks like storytelling or joke-telling, where you want some variety and creativity but still want the content to stay on topic.\n",
    "\n",
    "The API call sends the prompts along with the temperature setting, allowing the model to generate a response that is both aligned with the system and user prompts and has a level of randomness/creativity appropriate for joke-telling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b340e7-edac-42b9-b01e-761a8f2cecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d631f6-3ba9-4c01-8ce5-4643602bafdb",
   "metadata": {},
   "source": [
    "### **Temperature and Determinism:**\n",
    "\n",
    "* **temperature=0.4** controls the level of creativity:\n",
    "* **Low temperature** means that the model’s output is consistent and likely to be similar if the same prompt is given multiple times. It chooses the most probable response paths, making it predictable.\n",
    "* This setting is suitable for scenarios where consistency and accuracy are more important than creativity (e.g., generating concise answers, programming support, or well-defined jokes).\n",
    "* The trade-off is that the jokes may become more formulaic or less surprising as the model leans towards predictable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69f0043a-2947-4be0-9364-5e24c54549fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the data had a lot of levels!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "# The lower temperature setting ensures that the model sticks closely to conventional, high-probability jokes that align with the topic.\n",
    "completion = openai.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = prompts,\n",
    "    temperature = 0.4\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1798a33-5e7f-463b-b851-b13c70be794e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variance in the relationship, and they couldn't find a good way to normalize it!\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API needs system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model = \"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens = 200, # Limit the maximum number of tokens the model can generate in its response.\n",
    "    temperature = 0.7,\n",
    "    # Unlike some other APIs, Claude’s API requires the system message to be provided separately from the user prompt.\n",
    "    system = system_message,\n",
    "    messages = [\n",
    "        {\"role\":\"user\",\"content\":user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# message.content[0].text:\n",
    "# message: The object returned by the Claude API call, containing the response and other relevant information.\n",
    "# content: An attribute that holds the generated messages. It is a list containing one or more message objects.\n",
    "# content[0]: Accesses the first message in the response (usually the only one unless the API is set to return multiple responses).\n",
    "# .text: Extracts the actual text of the message.\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec804ceb-cf24-4f14-ada5-019c4a85c9d8",
   "metadata": {},
   "source": [
    "## **Technical Details and Considerations**\n",
    "### **Streaming Advantages:**\n",
    "* **Real-Time Feedback:** Streaming enables immediate feedback, which can be beneficial for user interaction, especially in chat interfaces or applications where responsiveness is crucial.\n",
    "* **Resource Management:** The context manager **(with result as stream:)** ensures that the streaming session is managed efficiently, closing the stream properly once the data is fully received.\n",
    "\n",
    "### **Temperature and Creativity:**\n",
    "* The **temperature value (0.7)** is kept the same as before, ensuring a balance between creative and consistent output while streaming. It provides diversity in the response but keeps it relevant to the user prompt and system context.\n",
    "\n",
    "### **Chunked Output:**\n",
    "* The **stream.text_stream** returns chunks of text, which might not always be complete sentences. This can give the impression of the model \"typing\" or generating content on the fly, similar to human interaction.\n",
    "* The **print()** function’s use of **flush=True** ensures the text is displayed as soon as it's received, preventing delays that might occur due to buffering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02691763-f651-43eb-b814-5165aa9467a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "Because they had too many null hypotheses!\n",
      "\n",
      "(Get it? Null hypothesis is a statistical concept, but it sounds like \"no love this is\" when said quickly!)\n",
      "\n",
      "This joke plays on the statistical term \"null hypothesis\" while also touching on the stereotypical image of data scientists being more focused on their work than their personal lives. It's a bit nerdy, but should get a chuckle from a data science crowd!"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet again\n",
    "# Now let's add in streaming back results\n",
    "result = claude.messages.stream(\n",
    "    model = \"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens = 200,\n",
    "    temperature = 0.7,\n",
    "    # In Claude’s API, the system message is provided separately from the user prompt, setting up the context before processing the user’s input.\n",
    "    system = system_message,\n",
    "    messages = [\n",
    "        {\"role\":\"user\",\"content\":user_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "# This block sets up a context manager to handle the streaming object (result) and iterates over the text as it's streamed back from the model.\n",
    "\n",
    "# The with statement is used to create a context manager for the stream object. \n",
    "# This ensures that resources are managed properly (e.g., closing the stream) when the block is exited.\n",
    "# The result object (returned by claude.messages.stream()) is treated as a stream, which contains the real-time output from the model.\n",
    "with result as stream:\n",
    "    # The stream.text_stream attribute provides an iterable that returns chunks of text as they are generated by the model. \n",
    "    # This allows the output to be processed and displayed piece by piece.\n",
    "    for text in stream.text_stream:\n",
    "        # The print() function outputs each chunk of text as it is received.\n",
    "        # end=\"\": Ensures that no newline is added after each chunk, so the output appears as a continuous stream.\n",
    "        # flush=True: Forces the output buffer to flush immediately, ensuring that each chunk is displayed in real-time, providing a smooth, incremental display of the text.\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7eef0e6-f1ad-4621-a727-cb9d67bcbb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the algorithm? \n",
      "\n",
      "Because it was always trying to **regress** their relationship! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The API for Gemini has a slightly different structure\n",
    "\n",
    "# Initialize a new instance of the GenerativeModel class from Google’s Generative AI library, specifically the Gemini model.\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name = \"gemini-1.5-flash\",\n",
    "    system_instruction = system_message\n",
    ")\n",
    "\n",
    "#Call the generate_content method of the gemini object, passing the user prompt to generate a response based on the model's configuration and instruction.\n",
    "response = gemini.generate_content(user_prompt)\n",
    "\n",
    "# Accesse the .text attribute of the response object and prints the generated content to the console.\n",
    "# response: The object returned by the generate_content method, containing the generated output from the model and possibly other related information such as tokens used or response time.\n",
    "# .text: The actual text content of the response. This is the main output generated by the model in response to the user prompt.\n",
    "print(response.text) # The .text attribute extracts the relevant text from the response object, making it accessible for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "106a22c7-503c-4eb3-87b9-134ec6acfe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution?\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54079706-88c1-4a6b-9ae6-856c74fddddd",
   "metadata": {},
   "source": [
    "### **Technical Details and Considerations**\n",
    "#### **Streaming Efficiency:**\n",
    "* **Real-Time Feedback:** Streaming allows the user to see the response as it is being generated, reducing perceived waiting time and enhancing interactivity.\n",
    "* **Efficiency:** By updating the display incrementally, users can start reading and engaging with the response even before it’s complete.\n",
    " \n",
    "#### **Markdown Display:**\n",
    "* The use of **Markdown** ensures that the response is formatted properly, allowing for headers, bullet points, bold text, and other formatting features to be applied dynamically as the text is streamed.\n",
    "* This is particularly useful for creating engaging content, such as instructions, lists, or structured explanations.\n",
    " \n",
    "#### **Data Handling and Processing:**\n",
    "* The **chunk.choices[0].delta.content** provides access to the new text generated by the model since the last chunk. The code efficiently accumulates this text into the **reply** string to build the complete response over time.\n",
    "* The **replace()** method is used to clean up any unwanted formatting artifacts that might interfere with the display, ensuring the output remains clean and user-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae8cf560-f254-4e88-8d9f-ebe844418b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Determining whether a business problem is suitable for a Large Language Model (LLM) solution involves evaluating several factors. Here are the key considerations:\n",
       "\n",
       "1. **Nature of the Problem**:\n",
       "   - **Textual Data**: LLMs are particularly effective for problems involving text. If your problem involves generating, understanding, summarizing, or translating text, an LLM might be suitable.\n",
       "   - **Conversational Interfaces**: If the solution requires a chatbot or virtual assistant, LLMs can be quite effective.\n",
       "   - **Content Creation**: Tasks involving writing copy, generating content ideas, or automating report generation can be well-suited for LLMs.\n",
       "\n",
       "2. **Complexity and Ambiguity**:\n",
       "   - **Complex Language Understanding**: If the problem requires deep understanding of context, nuance, or complex sentence structures, LLMs can be advantageous due to their advanced language understanding capabilities.\n",
       "   - **Handling Ambiguity**: LLMs can handle cases where language is ambiguous or context-dependent, which can be challenging for simpler models.\n",
       "\n",
       "3. **Data Availability**:\n",
       "   - **Pre-existing Text Data**: Access to a large corpus of relevant text data can improve the performance of LLMs, especially if fine-tuning is needed.\n",
       "   - **Privacy and Sensitivity**: Consider data privacy and compliance requirements, as LLMs may require access to sensitive information.\n",
       "\n",
       "4. **Scalability and Cost**:\n",
       "   - **Resource Requirements**: LLMs can be resource-intensive, requiring significant computational power. Evaluate whether your organization can support these requirements.\n",
       "   - **Cost-effectiveness**: Consider the cost of deploying and maintaining an LLM solution against the expected benefits.\n",
       "\n",
       "5. **Solution Requirements**:\n",
       "   - **Accuracy and Reliability**: Assess whether the level of accuracy provided by LLMs meets the business needs. While LLMs can be highly accurate, they are not infallible and may produce incorrect or biased outputs.\n",
       "   - **Customizability**: Determine if the model can be fine-tuned or customized to meet specific business requirements.\n",
       "\n",
       "6. **Ethical and Compliance Considerations**:\n",
       "   - **Bias and Fairness**: Consider the potential for biased outputs and ensure there are measures to mitigate these risks.\n",
       "   - **Regulatory Compliance**: Ensure that the use of LLMs complies with regulations relevant to your industry, such as GDPR for data protection.\n",
       "\n",
       "7. **Integration and Usability**:\n",
       "   - **Integration with Existing Systems**: Evaluate how well an LLM solution can integrate with your current systems and workflows.\n",
       "   - **User Acceptance**: Consider whether the end-users are likely to accept and effectively use the LLM-based solution.\n",
       "\n",
       "If a business problem aligns well with these factors, it may be suitable for a solution involving an LLM. However, it’s crucial to conduct a thorough analysis, possibly starting with a pilot project, to validate the effectiveness of an LLM in addressing your specific business needs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model = 'gpt-4o',\n",
    "    messages = prompts,\n",
    "    temperature = 0.7,\n",
    "    # Enable streaming mode, meaning the model’s response will be sent back incrementally instead of as a single chunk.\n",
    "    stream = True\n",
    ")\n",
    "\n",
    "# Initializes an empty string (reply) to store the streamed content and sets up a display handle for updating the output in markdown format.\n",
    "reply = \"\"\n",
    "\n",
    "# Markdown(\"\"): Creates an empty markdown object, which will be updated as new chunks arrive.\n",
    "# display_id=True: Creates a unique identifier (display_handle) for the markdown display, allowing updates to this display as the output streams in.\n",
    "display_handle = display(Markdown(\"\"), display_id = True)\n",
    "\n",
    "# This loop iterates over each chunk received from the streaming session, appending it to the reply string and updating the display in real-time\n",
    "for chunk in stream: #Iterate over the streamed response chunks as they are generated.Each chunk represents a small part of the response from the model.\n",
    "    # Accesse the content of the current chunk. The delta attribute contains the new content generated since the last update.\n",
    "    # The code uses or '' to ensure that if there’s no content in a chunk, it doesn't break the process.\n",
    "    # The content is appended to the reply string to build up the full response incrementally.\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    # Removes any extraneous markdown formatting markers (\"```\") or the word \"markdown\" that may appear. \n",
    "    reply = reply.replace(\"'''\", \"\").replace(\"markdown\", \"\")\n",
    "    # Updates the display with the current accumulated content in reply formatted as markdown.\n",
    "    # The display_id ensures the existing display is updated rather than creating a new one, providing a seamless, continuous display experience.\n",
    "    update_display(Markdown(reply), display_id = display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77a28a-ec0a-4df9-9fad-e0415c82d2ab",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f87dc9e-9adb-4ff4-b050-e878d0bab554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22b907-f786-4b2b-84f0-0b268cbd3643",
   "metadata": {},
   "source": [
    "### **Technical Details and Considerations**\n",
    "#### **System Message:**\n",
    "* The system message sets the overall behavior and context for the model, ensuring consistency in its responses. It’s crucial for guiding the model on how to respond throughout the session.\n",
    "  \n",
    "#### **Pairing Messages:**\n",
    "* Using **zip()** to pair **gpt_messages** and **claude_messages** ensures that the conversation alternates between the assistant (GPT) and the user (Claude), creating a coherent back-and-forth structure.\n",
    "  \n",
    "#### **API Call:**\n",
    "* The API call uses the model name stored in **gpt_model** and sends the accumulated conversation **(messages)** to generate a response. Make sure that **gpt_model** is defined correctly and the appropriate API key is set up to authenticate the request.\n",
    "  \n",
    "#### **Return Mechanism:**\n",
    "* The function returns the content of the model’s response, which can be used or displayed further, depending on the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46ede557-a029-4e12-9a52-e6276bc495ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    # The system message sets the initial behavior and context for the model, guiding it on how to respond throughout the conversation.\n",
    "    # The system message ensures that all subsequent interactions adhere to the model’s behavior and tone as set at the beginning of the session.\n",
    "    messages = [{\"role\":\"system\",\"content\":gpt_system}] \n",
    "    # This loop builds a conversation sequence where each GPT message (as the assistant) is followed by a user message (from Claude), creating a structured back-and-forth conversation.\n",
    "    # zip(gpt_messages, claude_messages): This function combines the two lists (gpt_messages and claude_messages) into pairs. \n",
    "    # Each pair consists of one message from gpt_messages and one from claude_messages.\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\":\"assistant\",\"content\":gpt}) # Add a message from the assistant (GPT) based on the gpt content.\n",
    "        messages.append({\"role\":\"user\",\"content\":claude}) # Add a user message based on the claude content, simulating user input or a counterpart response\n",
    "    # Making the API Call: Constructe messages list to the OpenAI API using the specified model (gpt_model).\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = gpt_model,\n",
    "        messages = messages\n",
    "    )\n",
    "    # Returning the Generated Response\n",
    "    return completion.choices[0].message.content # The .message.content attribute contains the text of the generated response from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ea63df8-fb4e-4416-8be7-8e7cc74c5a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, wow, a simple \"hi.\" How original! What’s next? A waving emoji?'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "710a0dc9-576a-46e4-ab03-fec28f88075d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\":\"user\",\"content\":gpt})\n",
    "        messages.append({\"role\":\"assistant\",\"content\":claude_message})\n",
    "    messages.append({\"role\":\"user\",\"content\":gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model = claude_model,\n",
    "        system = claude_system,\n",
    "        messages = messages,\n",
    "        max_tokens = 500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "43ab8e61-17b7-4c57-94d0-5e5f76ee09a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, it's nice to meet you! How are you doing today?\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "211bad1d-114b-4137-bf53-da273b34d65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Just what I needed—another greeting. What’s next, a weather update?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728dffc7-4032-4196-b095-e2d91bd48d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

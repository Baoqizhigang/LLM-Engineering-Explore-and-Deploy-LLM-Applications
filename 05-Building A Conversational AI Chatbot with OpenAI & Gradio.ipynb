{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80e5dbc-5715-4a04-bfed-3bddb5626f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f251103-c166-4125-9227-0280602ff55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b91627-dfdb-4101-9d37-aefc9a555633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5512561a-c543-4754-bc99-290f460c9d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c4def-6f30-498a-90c3-acbd1723e42f",
   "metadata": {},
   "source": [
    "## Reminder of the structure of prompt messages to OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "We will write a function `chat(message, history)` where:\n",
    "**message** is the prompt to use\n",
    "**history** is a list of pairs of user message with assistant's reply\n",
    "\n",
    "```\n",
    "[\n",
    "    [\"user said this\", \"assistant replied\"],\n",
    "    [\"then user said this\", \"and assistant replied again],\n",
    "    ...\n",
    "]\n",
    "```\n",
    "We will convert this history into the prompt style for OpenAI, then call OpenAI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe2353a0-df57-4ab1-9410-45689ea646b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    # message: The latest message input from the user.\n",
    "    # history: A list of tuples containing previous user and assistant messages.\n",
    "\n",
    "    # Initialize the conversation with a system message that sets the assistant's behavior.\n",
    "    # The OpenAI API expects messages as a list of dictionaries with \"role\" and \"content\" keys.\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "\n",
    "    # The conversation history is stored as a list of tuples, where each tuple contains a user message and the corresponding assistant reply.\n",
    "    for user_message, assistant_message in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": assitant_message})\n",
    "    # Add the latest user message to the conversation.\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    print(\"History is: \")\n",
    "    print(history)\n",
    "    print(\"And messages is: \")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model = MODEL, messages = messages, stream = True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        yield response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e941651-2ba4-49ce-bd9a-e6c50edf98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7888\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is: \n",
      "[]\n",
      "And messages is: \n",
      "[{'role': 'system', 'content': 'You are a helpful assistant'}, {'role': 'user', 'content': 'Hi, please introduce yourself to me. '}]\n"
     ]
    }
   ],
   "source": [
    "# gr.ChatInterface: A class provided by the Gradio library specifically designed for creating chat-based interfaces.\n",
    "# gr.ChatInterface(fn=chat): Creates a chat interface bound to your chat function, which processes messages.\n",
    "# .launch(): Starts the Gradio app, making the chat interface accessible to users.\n",
    "gr.ChatInterface(fn = chat, type = \"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93909b-fe8b-4a17-913a-ed4c0f31202a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5950a0e-6b15-4657-af8a-c0ff7c1184e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e638ce-b28b-4cbb-a27e-0c7cd044ae38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998f675-0301-47bc-bac4-7558e1bef3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5db10-fc1e-41f2-b41d-1a628580e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "({\"role\": \"\", \"\": })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffebb87-2499-440f-8668-b86643c6a5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a3df4-5178-46ab-9987-ed9776580de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fccfcd-9baf-4a88-9a04-429f10d73fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe0d0d2-5ff7-4e6a-9aeb-4cf0d1640ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be9ac5b-88e5-4283-9c26-de278895fad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

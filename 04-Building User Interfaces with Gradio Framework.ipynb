{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beefea68-27e2-4e0a-a19a-d81e9c39ec49",
   "metadata": {},
   "source": [
    "# Gradio Day!\n",
    "\n",
    "Today we will build User Interfaces using the outrageously simple Gradio framework.\n",
    "\n",
    "Prepare for joy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c706fa-20ae-42ca-a7ee-261af68f1411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The requests library is used to make HTTP requests to retrieve the contents of a web page. \n",
    "# In this project, it will be used to fetch the HTML of the page that needs to be summarized.\n",
    "# This library simplifies making HTTP requests, such as GET and POST, handling network errors, \n",
    "# and providing access to response content in different formats (text, JSON, etc.).\n",
    "# This step is essential to retrieve and prepare text data before summarization using GPT-4.\n",
    "import requests\n",
    "\n",
    "# BeautifulSoup is part of the bs4 library and is used to parse and extract data from HTML and XML documents. \n",
    "# After fetching the web page content using requests, BeautifulSoup helps extract clean, readable text from the raw HTML, \n",
    "# which can then be summarized by the GPT-4 model.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Python's typing module allows you to specify types for variables & function arguments, improving code readability and making it easier to catch errors.\n",
    "# Certain functions in this project expect lists as input, providing more clarity on data structures passed between functions.\n",
    "from typing import List\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3e6346-cab7-4fa8-8261-8208dfd20088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pydantic\n",
      "Version: 1.10.7\n",
      "Summary: Data validation and settings management using python type hints\n",
      "Home-page: https://github.com/pydantic/pydantic\n",
      "Author: Samuel Colvin\n",
      "Author-email: s@muelcolvin.com\n",
      "License: MIT\n",
      "Location: D:\\Programms\\Lib\\site-packages\n",
      "Requires: typing-extensions\n",
      "Required-by: anaconda-cloud-auth, anthropic, fastapi, google-generativeai, gradio, openai\n"
     ]
    }
   ],
   "source": [
    "# Run the following code in your Jupyter environment to check the installed version of Pydantic\n",
    "# If you have Pydantic v2.x installed, try downgrading it to a 1.x version, which is likely compatible with Gradio.\n",
    "!pip show pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43732a71-6b61-441b-8e42-c8ebae18be77",
   "metadata": {},
   "source": [
    "To install Pydantic to version 1.x(compatible version of Pydantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fd8e5d-8610-42b9-8987-488725f83cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic==1.10.7 in d:\\programms\\lib\\site-packages (1.10.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\programms\\lib\\site-packages (from pydantic==1.10.7) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic==1.10.7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23de0970-9930-4e19-b319-baadabde82c8",
   "metadata": {},
   "source": [
    "Once Pydantic is set to a compatible version, you can also reinstall Gradio to ensure everything works smoothly\n",
    "If you already have gradio installed, run this code. Otherwise, skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b90e5-777d-4dcb-a373-942a83106b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550a479-a3a1-4300-a530-a49e57d34f07",
   "metadata": {},
   "source": [
    "After installing the correct version of Pydantic and Gradio, restart the Jupyter kernel to apply the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35831e58-dce1-4af7-9160-28e70c286445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in d:\\programms\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\programms\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in d:\\programms\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0 in d:\\programms\\lib\\site-packages (from gradio) (0.115.2)\n",
      "Requirement already satisfied: ffmpy in d:\\programms\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in d:\\programms\\lib\\site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in d:\\programms\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in d:\\programms\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\programms\\lib\\site-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in d:\\programms\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in d:\\programms\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in d:\\programms\\lib\\site-packages (from gradio) (3.10.9)\n",
      "Requirement already satisfied: packaging in d:\\programms\\lib\\site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\programms\\lib\\site-packages (from gradio) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in d:\\programms\\lib\\site-packages (from gradio) (10.2.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Requirement already satisfied: pydub in d:\\programms\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in d:\\programms\\lib\\site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in d:\\programms\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.2.2 in d:\\programms\\lib\\site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in d:\\programms\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in d:\\programms\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in d:\\programms\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in d:\\programms\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in d:\\programms\\lib\\site-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: fsspec in d:\\programms\\lib\\site-packages (from gradio-client==1.4.0->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in d:\\programms\\lib\\site-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\programms\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\programms\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: starlette<0.41.0,>=0.37.2 in d:\\programms\\lib\\site-packages (from fastapi<1.0->gradio) (0.40.0)\n",
      "Requirement already satisfied: certifi in d:\\programms\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programms\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\programms\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in d:\\programms\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in d:\\programms\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\programms\\lib\\site-packages (from huggingface-hub>=0.25.1->gradio) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programms\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programms\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\programms\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\programms\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\programms\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\programms\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\programms\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\programms\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.3.5)\n",
      "Requirement already satisfied: colorama in d:\\programms\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programms\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\programms\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\programms\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programms\\lib\\site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programms\\lib\\site-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.0.7)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\programms\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.7\n",
      "    Uninstalling pydantic-1.10.7:\n",
      "      Successfully uninstalled pydantic-1.10.7\n",
      "Successfully installed pydantic-2.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\Programms\\Lib\\site-packages\\~-dantic'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-cloud-auth 0.1.4 requires pydantic<2.0, but you have pydantic 2.9.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74bc36-22b6-4d70-94f8-e0195c358ad0",
   "metadata": {},
   "source": [
    "## Gradio Context:\n",
    "\n",
    "**Why Gradio?:** \n",
    "* When working with machine learning, especially LLMs, you often want an interface to interact with models without writing extensive code. Gradio simplifies this process by allowing developers to quickly build UIs, test models, and share them easily. It's often used in rapid prototyping of models for user feedback or demonstrations.\n",
    "* You can use Gradio to test LLM outputs in real-time by providing prompts and receiving responses directly on the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af88d134-9ab6-412f-9bfe-0ab4b1b774ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio is used for building user-friendly interfaces for ML models. \n",
    "# With Gradio, you can quickly create input/output interfaces (for text, images, audio, etc.) for your machine learning models.\n",
    "import gradio as gr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a958b74-fa7e-4261-aa11-ae40ea744ae4",
   "metadata": {},
   "source": [
    "**Environment variables** provide a layer of security and flexibility, as sensitive data (API keys, database URLs, etc.) is kept separate from the code. This is essential when dealing with services like OpenAI, Anthropic, or Google, which require API keys for authentication and access.\n",
    "\n",
    "**LLM Application Workflow:** \n",
    "\n",
    "* User sends a query or prompt through a Gradio UI.\n",
    "* The backend fetches the appropriate API key and sends the request to the LLM provider.\n",
    "* The LLM processes the prompt and returns a response, which is then displayed in the Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fecdd678-eaa4-49ac-a6ca-533b06d46d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # load environment variables from a .env file into the program’s environment. \n",
    "\n",
    "# os.getenv() looks for an environment variable (like OPENAI_API_KEY). If it exists, the function retrieves its value. \n",
    "# If not, it returns the default value provided, ensuring that your program doesn't crash if the .env file is missing.\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv('ANTHROPIC_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30bd6792-0175-451c-9f59-cc114b9606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "\n",
    "openai = OpenAI() # Initialize the OpenAI client, which will allow you to interact with OpenAI’s API (e.g., for GPT-4 or other models).\n",
    "\n",
    "# Anthropic() class initializes a connection to Anthropic’s API using the key that was set in the environment variables.\n",
    "# With this object, you can query models like Claude for tasks such as conversational AI, text generation, and summarization.\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a7a24-d8d8-4242-a5e4-e4ee6e4d42ad",
   "metadata": {},
   "source": [
    "**System messages** are an essential part of the **prompt engineering** process when interacting with LLMs. They provide context or instructions to shape how the model generates responses. The message usually defines the role or behavior the LLM should adopt while interacting with the user.\n",
    "\n",
    "**Prompt Engineering:** This technique is crucial when fine-tuning the behavior of LLMs. System messages help ensure that the model adheres to a specific tone, role, or set of expectations. It essentially functions as an invisible guide for the model’s behavior, ensuring that all generated responses stay aligned with the desired assistant-like behavior.\n",
    "\n",
    "**Role-Playing:** You can change the system message to define different behaviors or roles, such as “You are a sarcastic assistant” or “You are a financial advisor.” This is part of role-playing with LLMs, which is commonly used in chatbot design and conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb7dce79-74e0-432e-b8cf-3e4127576b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system message that will guide the behavior of the language model (LLM) in its responses. \n",
    "# The LLM interprets this message before processing any user input and uses it as a guiding principle throughout the conversation.\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba779d62-69cb-4e0f-94c3-f86f32b0d802",
   "metadata": {},
   "source": [
    "**System Message:** Guides the behavior of the LLM, making it act like a helpful assistant.\n",
    "\n",
    "**User Prompt:** The specific input from the user to which the LLM will respond.\n",
    "\n",
    "**OpenAI API:** The interface used to communicate with and get responses from the GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b406b51-333c-491c-ae64-acac0b8a9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt):\n",
    "    # Create the Message Structure: This message structure is typical in chat-based models like GPT-3.5 or GPT-4, \n",
    "    # where multiple roles (system, user, assistant) participate in the conversation.\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message }, # A system message instructs the model how to behave.\n",
    "        {\"role\":\"user\", \"content\": prompt } # A user message is the actual prompt provided by the user,the specific input to which the model will respond.\n",
    "    ]\n",
    "    \n",
    "    # Calling the GPT-4o-mini Model by using the OpenAI API’s chat.completions.create() method.\n",
    "    # The openai.chat.completions.create() method is responsible for querying the API and retrieving the completion (the model’s response).\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = 'gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "    # The API’s response is usually structured into choices. Each choice represents a potential reply generated by the model.\n",
    "    # The function accesses the first choice (choices[0]) and retrieves the message.content, which contains the generated text.\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b221f30b-e157-488f-840f-ab4a236be086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is October 2, 2023.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d8817-b29a-490c-81cf-0e22f541d830",
   "metadata": {},
   "source": [
    "Since GPT-4o-mini does not have real-time access to the current date, it will most likely respond with a general answer or an estimate, and it may not give the accurate current date unless it was fine-tuned for that use case.\n",
    "## To get the actual date:\n",
    "* If you're looking for real-time functionality like fetching the actual current date, you'll need to integrate Python's datetime module with the function or an external real-time API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cfa0ae0-b7d9-4c9b-8211-bc5e847c479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date is 2024-10-19.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    if \"date\" in prompt.lower():\n",
    "        current_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        return f\"Today's date is {current_date}.\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Example call\n",
    "response = message_gpt(\"What is today's date?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf853535-22f3-47da-9c57-dc595edc4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    # upper() method is a built-in Python string method that converts all lowercase letters in a string to uppercase.\n",
    "    return text.upper()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebac877-80d1-4ab3-812c-e2e20ffcaa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shout(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599c8da6-73f2-41df-99b3-a4f5ddf5351a",
   "metadata": {},
   "source": [
    " ## **Basic Gradio Interface**\n",
    "**Purpose:**\n",
    "\n",
    "This creates a simple Gradio interface that allows users to interact with the shout function through a web-based UI.\n",
    "The interface has a textbox for input (where users will type in a string) and another textbox for output (which will display the uppercase version of the input).\n",
    "\n",
    "* **fn=shout:** Specifies that the function to be called when the user submits input is shout.\n",
    "* **inputs=\"textbox\":** This indicates that the user will provide input through a textbox.\n",
    "* **outputs=\"textbox\":** The output (uppercased version of the input) will also be displayed in a textbox.\n",
    "* **.launch():** This launches the Gradio interface, allowing users to interact with it in a web browser. A local web server is started, and a link is provided where the interface can be accessed.\n",
    "#### Sharing Publicly:\n",
    "* With **share=True**, Gradio will generate a link (such as https://random-link.gradio.app) that you can share with others, allowing them to interact with your interface without needing to install anything or run the code themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019f1743-4deb-43ba-b503-6859ecff74a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input What'sup yo\n",
      "Shout has been called with input i love this tutorial\n"
     ]
    }
   ],
   "source": [
    "#  Create a simple Gradio interface that allows users to interact with the shout function through a web-based UI.\n",
    "gr.Interface(fn=shout, \n",
    "             inputs = \"textbox\", \n",
    "             outputs = \"textbox\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a36035-faa5-4edf-b95c-a8d97f18b2f4",
   "metadata": {},
   "source": [
    "Disables flagging and provides a public URL for others to test the interface remotely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242174e2-c615-48f9-8f29-05bdb86e4e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://3e2f024819c8e7b0ec.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3e2f024819c8e7b0ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input i love coding, yeah!\n"
     ]
    }
   ],
   "source": [
    "# The flagging_mode=\"never\" argument disables the flagging feature. \n",
    "# By default, Gradio allows users to flag results if they believe there is an issue (e.g., incorrect output). \n",
    "gr.Interface(fn=shout, \n",
    "             inputs=\"textbox\", \n",
    "             outputs=\"textbox\", \n",
    "             flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62e4b87-f786-42d9-a460-de867b6ce873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input I love coding, yeah!\n"
     ]
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn=shout, # Specifie that the shout function will be used\n",
    "    # The input field is a textbox where the user can type their message. It’s labeled \"Your message:\" and allows up to 6 lines of text.\n",
    "    inputs = [gr.Textbox(label = \"Your message: \", lines = 6)],\n",
    "    # The output will be displayed in another textbox, labeled \"Response:\", with space for 8 lines of text.\n",
    "    outputs = [gr.Textbox(label = \"Response: \", lines = 8)],\n",
    "    flagging_mode = \"never\")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3e23f-cb16-438d-ba0e-574883011e53",
   "metadata": {},
   "source": [
    "## Summary of the Two Interfaces:\n",
    "* **Interface 1 (shout):** Converts user input to uppercase and displays it in the response box. Useful for simple text transformations.\n",
    "* **Interface 2 (message_gpt):** Sends user input to the GPT-4o-mini model and displays the AI-generated response. Suitable for interacting with an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ac0b780-9d31-4711-b9a4-be2b4e694477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = message_gpt,\n",
    "    inputs = [gr.Textbox(label = \"Your message: \")],\n",
    "    outputs = [gr.Textbox(label = \"Response: \")],\n",
    "    flagging_mode = \"never\"\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de8fc5d2-d0e1-4ef6-b5b3-98b4c2d5c80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn = message_gpt,\n",
    "    inputs = [gr.Textbox(label = \"Your message: \")],\n",
    "    outputs = [gr.Markdown(label = \"Response: \")], # The output is now rendered in Markdown format. \n",
    "    flagging_mode = \"never\"\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f329abdd-5c46-41eb-8a30-fcf387720fd2",
   "metadata": {},
   "source": [
    "## Yield vs Return:\n",
    "\n",
    "yield makes this a generator function. Instead of returning a single result after the entire response is generated, it yields intermediate results as they become available.This is what allows the function to stream the results back to the user continuously.\n",
    "\n",
    "## How It Works in Practice:\n",
    "When **def stream_gpt(prompt):** is used in a Gradio interface or other interactive environment, the response is displayed to the user incrementally as it’s generated by the GPT-4o-mini model.As soon as the first part of the response is ready, it appears on the screen, followed by subsequent parts, creating a dynamic and responsive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2076d6db-5984-4410-ae31-5e7faa74ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming GPT-4o-mini Results\n",
    "\n",
    "def stream_gpt(prompt):\n",
    "    # Message Structure\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    \n",
    "    # Calling the GPT-4o-mini API with Streaming\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True # The stream=True argument enables streaming mode\n",
    "    )\n",
    "    \n",
    "    result = \"\" # result is initialized as an empty string, and each chunk is added to it as the stream progresses.\n",
    "\n",
    "    # The stream is an iterator that sends chunks of the response back as they are generated.\n",
    "    for chunk in stream:\n",
    "        # or \"\": If delta.content is None (which can happen for some chunks), it adds an empty string instead to avoid errors.\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        # The yield statement sends back the current state of the result at each step, allowing the function to continuously output the growing result in real time.\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32ed5381-c89e-47a8-a1ca-043db498dd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7872\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7872/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_gpt,\n",
    "    inputs = [gr.Textbox(label = \"Your message: \")],\n",
    "    outputs = [gr.Markdown(label = \"Response: \")],\n",
    "    flagging_mode = \"never\",\n",
    "    #live = True # 'live=True' is used for real-time feedback\n",
    ")\n",
    "\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11d491eb-ee3c-483b-9c39-92ce57abe8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_claude(prompt):\n",
    "    result = claude.messages.stream(\n",
    "        model = \"claude-3-haiku-20240307\",\n",
    "        max_tokens = 1000,\n",
    "        temperature = 0.7,\n",
    "        system = system_message,\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    response = \"\"\n",
    "    \n",
    "    # Set up the streaming of the response, meaning Claude will start generating and sending back parts of the response in real-time.\n",
    "    with result as stream:\n",
    "        # As each chunk of the response is streamed back (via stream.text_stream), it is concatenated to the response string. \n",
    "        # The text or \"\" ensures that even if a chunk is empty or None, it won't break the process.\n",
    "        for text in stream.text_stream:\n",
    "            response += text or \"\"\n",
    "            # The yield response statement sends the current state of the response to the caller at every iteration.\n",
    "            yield response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1533881e-2fd6-42b2-b96e-eb8035895f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_claude,\n",
    "    inputs = [gr.Textbox(label = \"Your message: \")],\n",
    "    outputs = [gr.Markdown(label = \"Response: \")],\n",
    "    flagging_mode = \"never\"   \n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "067bec9b-e249-478e-8cc6-1b0f623f97ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt, model):\n",
    "    if model == \"GPT\":\n",
    "        result = stream_gpt(prompt)\n",
    "    elif model == \"Claude\":\n",
    "        result = stream_claude(prompt)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model\")\n",
    "        \n",
    "    #  Once the appropriate model is selected and called (either GPT or Claude), the response is streamed back in chunks.\n",
    "    for chunk in result:\n",
    "        # The yield statement is used to continuously provide the chunks of the result back to the user. \n",
    "        # As each chunk of the response is generated, it is yielded, allowing the UI (or any other consumer) to display the response in real time.\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca96342f-8d99-4994-9608-fa5751961261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_model,\n",
    "    inputs = [gr.Textbox(label = \"Your message: \"), gr.Dropdown([\"GPT\",\"Claude\"], label = \"Select model\")],\n",
    "    outputs = [gr.Markdown(label = \"Response: \")],\n",
    "    flagging_mode = \"never\"\n",
    ")\n",
    "\n",
    "view.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb578ea-1177-4007-a825-8bb3557f305e",
   "metadata": {},
   "source": [
    "## Building a company brochure generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "873e050d-4117-4dc5-925d-1b3c9bfe1658",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '：' (U+FF1A) (292981868.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[45], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    text： str\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '：' (U+FF1A)\n"
     ]
    }
   ],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    url: str\n",
    "    title: str\n",
    "    text： str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d63b71-fcd1-4678-a5b9-c66ceacb8194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df1654-c180-40d9-8ab8-e5b37f7a4345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb482d-3610-41e1-9c3f-3ccb197c6a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fab625-39ca-4304-a98b-0ac5b6e5fe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
